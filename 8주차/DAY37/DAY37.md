## Entropy

지금까지 손실함수로써 cross-entropy를 수없이 많이 접해왔지만 그 정확한 의미에 대해 제대로 생각해본적이 없다. 따라서 엔트로피와 크로스 엔트로피에 대해 이해하고, 각각의 식에 대한 해석을 하고자한다.

엔트로피란 불확실성(uncertainty)에 대한 척도이다. 큐브를 보면, 모든 면이 제대로 맞추어진 경우의 수는 1이지만 이를 흐뜨러뜨리기 시작하면 그 수를 바로 가늠하기 어려울정도로 많아진다. 이 때 큐브 입장에서 나중의 엔트로피가 더 높다고 볼 수 있다. 그 불확실성의 정도가 커지기 때문이다.

따라서 엔트로피의 경우 예측하기 어려운 일에서 더 높아진다. 예를 들어 주사위와 동전이 있을 때 엔트로피 식 $H(x)=−\sum _{i=1}^{n}  p(x_i) \log{p(x_i)}$
에 따라 그값은 아래와 같다.  
<br/>

$H(x)= -\left( \frac{1}{2} \log{\frac{1}{2}} + \frac{1}{2} \log{\frac{1}{2}} \right) \approx 0.693$  
$H(x)= - \left( \frac{1}{6} \log{\frac{1}{6}} + \cdots + \frac{1}{6} \log{\frac{1}{6}} \right) \approx 1.79$  

보다시피 불확실성이 더 큰 주사위의 엔트로피가 더 높다. 여담으로 위와 같이 모든 사건이 일어날 확률이 동일한 경우, 엔트로피는 결국 
$H(x) = -\log p(x_i)$가 된다. 즉, 모든 사건의 확률이 uniform하면 일어날 수 있는 사건의 갯수가 많을수록 엔트로피 값도 크다.