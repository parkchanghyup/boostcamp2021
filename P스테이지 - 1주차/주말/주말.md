# 프로젝트 회고 (토요일)

1. Competition을 위한 코드 진행 과정

    - 완성된 코드를 바탕으로 여러가지 하이퍼 파라미터를 수정하는중
    - 성능 향상에 효과가 있었던 것을 위주로 작성 해보면 아래와 같다.
        - 58세 이상 이미지를 60세 이미지로 라벨링 -> 60세이상 이미지 갯수가 너무 적어서 어쩔 수 없이 적용.  (이현규 캠퍼님 의견)
        - train / val 의 batch_size를 다르게 적용 -> 이후 test 를 진행할 때는 val 에 사용한 batch size를 적용
        - `optimizer = MADGRAD` 사용 (허재섭 캠퍼님 의견)
        - `lr_scheduler = CosineAnnealingWarmRestarts` 사용 -> f1 score 향상에 많은 도움이됨..
        - test data set에 TTA를 적용해줌 -> 성능 향상에 큰 기여는 하지 않았지만 미세하게 올라감. 추후에 더 실험 해볼 예정
        - K-FOLD -> 진리의 앙상블 그냥 성능 향상이 보장된거라 적을까 말까 하다가 그냥 적음.


2. 추후 진행할 사항
    - F1-loss 적용
    - 성능 향상에 효과적인 Augmentation 찾아 보기
    - 현재 b0 모델 사용중인데 더 무거운 모델 적용


# 프로젝트 회고 (일요일)

1. Competition을 위한 코드 진행 과정

    - 현재 최고 f1 score : 0.7373 
    - 적용한 기법 : K-fold(k=3) 앙상블, TTA,loss 함수에 weight , b0, cosine, Centercrop(300), age 58, CosineAnnealingWarmRestarts
    - 하이퍼 파라미터 : epcoh 10 , lr = 0.0001 ,train / val = 16, 8


2. 오늘 적용한 기법
    - f1 loss -> `f1-score : 0.6860`
    - train batch 32 -> `f1-score : 0.7069`
    - regnet_x 50 -> `f1-score : 0.6618`
    - age 59 ->  `f1-score : 0.6845`
    - val batch =4 -> `f1-score : 0.6689`
    - b4 -> `f1-score : 0.6167 `


3. 추후 진행할 사항
    - lr 스케쥴 바꿔보기
    - b0 말고 다른모델 사용
    - 성능 향상에 효과적인 Augmentation 찾아 보기

    
    